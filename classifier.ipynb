{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_directory = \"./news_dataset\"\n",
    "\n",
    "unique_docs_per_category = {}\n",
    "global_seen_docs = {}\n",
    "untitled_folder_count = 0\n",
    "unique_files = set()\n",
    "all_files = []\n",
    "content_by_category = {}\n",
    "for root, dirs, files in os.walk(root_directory):\n",
    "    if os.path.basename(root) == \"Untitled Folder\":\n",
    "        untitled_folder_count += 1\n",
    "        continue\n",
    "    relative_path = os.path.relpath(root, root_directory)\n",
    "    path_components = [\"Root\"] if relative_path == \".\" else relative_path.split(os.sep)\n",
    "    for component in path_components:\n",
    "        if component == \"Untitled Folder\" or component == \"Root\":\n",
    "            continue\n",
    "        if component not in unique_docs_per_category:\n",
    "            unique_docs_per_category[component] = 0\n",
    "            global_seen_docs[component] = set()\n",
    "            content_by_category[component] = {'titles': [], 'bodies': []}\n",
    "    files.sort(key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    for file in files:\n",
    "        unique_files.add(file)\n",
    "        all_files.append(file)\n",
    "        file_path = os.path.join(root, file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            title = lines[0].strip()\n",
    "            body = ''.join(lines[1:]).strip()\n",
    "        for category in path_components:\n",
    "            if category != \"Untitled Folder\" and file not in global_seen_docs[category]:\n",
    "                unique_docs_per_category[category] += 1\n",
    "                global_seen_docs[category].add(file)\n",
    "                content_by_category[category]['titles'].append(title)\n",
    "                content_by_category[category]['bodies'].append(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 113\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of categories: \" + str(len(unique_docs_per_category.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 17599\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents: \" + str(len(all_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique documents: 6539\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique documents: \" + str(len(unique_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Class</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>اجتماعی</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>اطلاعاتی</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>جو فرهنگی حاکم بر کشور</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...</td>\n",
       "      <td>فرهنگی امنیتی</td>\n",
       "      <td>تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...</td>\n",
       "      <td>تهدیدات اجتماعی و فرهنگی</td>\n",
       "      <td>تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>گروگانگیری در شعبه بانک انصار شوش با دستگیری گ...</td>\n",
       "      <td>مبارزه با گروگانگیری</td>\n",
       "      <td>دزفول- ایرنا- فرد مسلحی که صبح پنجشنبه با ورود...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>هشدار رئیس‌جمهور چین نسبت به تهدید‌های فزاینده...</td>\n",
       "      <td>مسولان عالی تصمیم گیر کشوری و لشگری</td>\n",
       "      <td>رییس‌جمهوری چین با تاکید بر اینکه چین با تهدید...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>آمریکا برای حمله به سوریه آماده می‌شود</td>\n",
       "      <td>پدافند هوایی</td>\n",
       "      <td>وزیر دفاع آمریکا اعلام کرد، پنتاگون در حال است...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>پنتاگون در آستانه تعطیل دولت، ۵ میلیارد دلار ت...</td>\n",
       "      <td>عملیات روانی و تهاجم فرهنگی دشمن</td>\n",
       "      <td>ب پیش از فرستاده شدن ۸۰۰ هزار کارمند دولت آمری...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>فعالیت احزاب و گروه ها</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "1    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "2    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "3    آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...   \n",
       "4    آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...   \n",
       "..                                                 ...   \n",
       "108  گروگانگیری در شعبه بانک انصار شوش با دستگیری گ...   \n",
       "109  هشدار رئیس‌جمهور چین نسبت به تهدید‌های فزاینده...   \n",
       "110             آمریکا برای حمله به سوریه آماده می‌شود   \n",
       "111  پنتاگون در آستانه تعطیل دولت، ۵ میلیارد دلار ت...   \n",
       "112  مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "\n",
       "                                   Class  \\\n",
       "0                                اجتماعی   \n",
       "1                               اطلاعاتی   \n",
       "2                 جو فرهنگی حاکم بر کشور   \n",
       "3                          فرهنگی امنیتی   \n",
       "4               تهدیدات اجتماعی و فرهنگی   \n",
       "..                                   ...   \n",
       "108                 مبارزه با گروگانگیری   \n",
       "109  مسولان عالی تصمیم گیر کشوری و لشگری   \n",
       "110                         پدافند هوایی   \n",
       "111     عملیات روانی و تهاجم فرهنگی دشمن   \n",
       "112               فعالیت احزاب و گروه ها   \n",
       "\n",
       "                                                  Body  \n",
       "0    خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "1    خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "2    خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "3    تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...  \n",
       "4    تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...  \n",
       "..                                                 ...  \n",
       "108  دزفول- ایرنا- فرد مسلحی که صبح پنجشنبه با ورود...  \n",
       "109  رییس‌جمهوری چین با تاکید بر اینکه چین با تهدید...  \n",
       "110  وزیر دفاع آمریکا اعلام کرد، پنتاگون در حال است...  \n",
       "111  ب پیش از فرستاده شدن ۸۰۰ هزار کارمند دولت آمری...  \n",
       "112  خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "\n",
       "[113 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `content_by_category` is the dictionary you're working with\n",
    "data_samples = []\n",
    "for category in content_by_category.keys():\n",
    "    contents = content_by_category[category]\n",
    "    title, body = contents['titles'][0],contents['bodies'][0]\n",
    "    displayed_body = (body[:47] + '...') if len(body) > 50 else body\n",
    "    data_samples.append({\"Title\": title, \"Class\": category, \"Body\": displayed_body})\n",
    "\n",
    "df_samples = pd.DataFrame(data_samples)\n",
    "\n",
    "display(df_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import hazm\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "normalizer = hazm.Normalizer()\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "punctuations = [')', '(', '>', '<', \"؛\", \"،\", '{', '}', \"؟\", ':', \"–\", '»', '\"', '«', '[', ']', '\"', '+', '=', '?', '/',\n",
    "                '//', '\\\\', '|', '!', '%', '&', '*', '$', '#', '؟', '*', '.', '_', '']\n",
    "persian_numbers = '۰۱۲۳۴۵۶۷۸۹'\n",
    "preprocessed_docs = []\n",
    "categories = []\n",
    "stopwords_list = hazm.stopwords_list()\n",
    "for category in content_by_category.keys():\n",
    "    category_text = \"\"\n",
    "    for doc in content_by_category[category]['bodies']:\n",
    "        normalized_text = normalizer.normalize(doc)\n",
    "        tokens = hazm.word_tokenize(normalized_text)\n",
    "        lemmatized_text = \"\"\n",
    "        for token in tokens:\n",
    "            if token in stopwords_list or token in punctuations or token.isdigit() or any(char in persian_numbers for char in token) or any(char in string.ascii_letters for char in token):\n",
    "                continue\n",
    "            lemmatized_token = lemmatizer.lemmatize(token)\n",
    "            lemma = lemmatized_token.split('#')[1] if '#' in lemmatized_token else lemmatized_token\n",
    "            lemmatized_text += lemmatized_token + \" \"\n",
    "        category_text += lemmatized_text + \" \"\n",
    "\n",
    "    preprocessed_docs.append(category_text.strip())\n",
    "    categories.append(category)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           آب  آب سنگین      آباد      آبان      آبرو       آتش    آتش بس  \\\n",
      "0    0.010637  0.009258  0.011642  0.011191  0.009849  0.012896  0.012492   \n",
      "1    0.010237  0.011273  0.009831  0.009486  0.009583  0.010195  0.011575   \n",
      "2    0.010620  0.000000  0.013370  0.008731  0.006329  0.014230  0.004835   \n",
      "3    0.011025  0.009353  0.011916  0.011582  0.009734  0.013433  0.012125   \n",
      "4    0.011802  0.000000  0.000000  0.007623  0.000000  0.011906  0.000000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "108  0.007738  0.000000  0.024653  0.000000  0.000000  0.007806  0.000000   \n",
      "109  0.015868  0.008029  0.013966  0.012953  0.011326  0.017887  0.021397   \n",
      "110  0.016087  0.010903  0.008513  0.012571  0.000000  0.015780  0.016751   \n",
      "111  0.014472  0.010626  0.008296  0.000000  0.018425  0.015605  0.017128   \n",
      "112  0.012412  0.000000  0.011093  0.009904  0.000000  0.014369  0.014615   \n",
      "\n",
      "      آتش زدن  آتش کشید    آتشبار  ...  یکشنبه اعلام  یکشنبه شب  یکشنبه گذشته  \\\n",
      "0    0.013885  0.011488  0.008734  ...      0.008147   0.010543      0.010462   \n",
      "1    0.008796  0.008487  0.008960  ...      0.009179   0.009692      0.008315   \n",
      "2    0.011058  0.000000  0.000000  ...      0.000000   0.013969      0.010082   \n",
      "3    0.015007  0.012416  0.009440  ...      0.008805   0.009633      0.010715   \n",
      "4    0.016347  0.008381  0.000000  ...      0.000000   0.013399      0.000000   \n",
      "..        ...       ...       ...  ...           ...        ...           ...   \n",
      "108  0.000000  0.000000  0.000000  ...      0.000000   0.010888      0.020507   \n",
      "109  0.000000  0.004834  0.008861  ...      0.011649   0.007729      0.000000   \n",
      "110  0.010331  0.008968  0.019982  ...      0.005348   0.008468      0.005563   \n",
      "111  0.012480  0.008740  0.009462  ...      0.005213   0.011632      0.005422   \n",
      "112  0.005978  0.012382  0.009511  ...      0.000000   0.012785      0.000000   \n",
      "\n",
      "     یکشنبه گزارش      یکصد       یکم   یکپارچه  یکپارچگی      یگان        یی  \n",
      "0        0.007547  0.009622  0.010163  0.009043  0.009585  0.010902  0.014355  \n",
      "1        0.008638  0.009010  0.011394  0.009642  0.008674  0.009772  0.010312  \n",
      "2        0.008979  0.008040  0.000000  0.004924  0.000000  0.004154  0.000000  \n",
      "3        0.007173  0.008544  0.010985  0.008861  0.009817  0.010953  0.014416  \n",
      "4        0.000000  0.007019  0.000000  0.000000  0.000000  0.016023  0.007914  \n",
      "..            ...       ...       ...       ...       ...       ...       ...  \n",
      "108      0.000000  0.000000  0.027878  0.000000  0.000000  0.008449  0.000000  \n",
      "109      0.000000  0.008497  0.009429  0.010020  0.007877  0.013971  0.015908  \n",
      "110      0.010398  0.013069  0.000000  0.016742  0.005097  0.013524  0.017430  \n",
      "111      0.004829  0.009073  0.005947  0.000000  0.004968  0.003782  0.013608  \n",
      "112      0.004854  0.007358  0.000000  0.007631  0.004993  0.010613  0.008296  \n",
      "\n",
      "[113 rows x 10000 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import hazm\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000,  ngram_range=(1,2),sublinear_tf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Class</th>\n",
       "      <th>Key words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم</td>\n",
       "      <td>اجتماعی</td>\n",
       "      <td>آسیب اجتماع, بچه پولدار, سبک زندگی, مخملی, جنگ نرم, اعتیاد, سوروس, پولدار, بافق, کودتا مخملی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم</td>\n",
       "      <td>اطلاعاتی</td>\n",
       "      <td>یوتا, موادمخدر, جزوه, بلک واتر, ابوطالب, بچه پولدار, برنامه اقدام, سالیوان, آستارا, واتر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم</td>\n",
       "      <td>جو فرهنگی حاکم بر کشور</td>\n",
       "      <td>بچه پولدار, پولدار, مشایی, انصار الله, عبدالملک الحوثی, جنبش انصار, حجاب, الحوثی, جمهور یمن, شهر صنعا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را جمع آوری کرد</td>\n",
       "      <td>فرهنگی امنیتی</td>\n",
       "      <td>آسیب اجتماع, بچه پولدار, مخملی, سبک زندگی, جنگ نرم, سوروس, اعتیاد, پولدار, کودتا مخملی, بافق</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را جمع آوری کرد</td>\n",
       "      <td>تهدیدات اجتماعی و فرهنگی</td>\n",
       "      <td>آسیب اجتماع, بچه پولدار, اعتیاد, پولدار, تهاجم فرهنگ, موادمخدر, التحریر, نرخ بیکاری, ازدواج, سبک زندگی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>گروگانگیری در شعبه بانک انصار شوش با دستگیری گروگانگیر پایان یافت</td>\n",
       "      <td>مبارزه با گروگانگیری</td>\n",
       "      <td>آزاد مرزبان, گروه جیش, جیش العدل, جمشید دانا, دانا فر, العدل, ذوالفقار, مرزبان ربوده, جمشید, مرزبان ایران</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>هشدار رئیس‌جمهور چین نسبت به تهدید‌های فزاینده علیه امنیت ملی</td>\n",
       "      <td>مسولان عالی تصمیم گیر کشوری و لشگری</td>\n",
       "      <td>فاینانس, انصار الله, عبدالملک الحوثی, آمرلی, جنبش انصار, دریابانی, مرزبان ناجا, قرارگاه پدافند, جمهور یمن, پوروشنکو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>آمریکا برای حمله به سوریه آماده می‌شود</td>\n",
       "      <td>پدافند هوایی</td>\n",
       "      <td>قرارگاه پدافند, موشک اروپا, هوا خاتم, فرمانده قرارگاه, دادستان آلمان, پیامبر اعظم, ایران, خاتم الانبیاء, الانبیاء, سامانه رادار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>پنتاگون در آستانه تعطیل دولت، ۵ میلیارد دلار تجهیزات نظامی خرید</td>\n",
       "      <td>عملیات روانی و تهاجم فرهنگی دشمن</td>\n",
       "      <td>جزوه, مدرسین, تهاجم فرهنگ, مشایی, بختیار, مصباح یزد, ایران, التحریر, حزب مشارکت, علمیه قم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم</td>\n",
       "      <td>فعالیت احزاب و گروه ها</td>\n",
       "      <td>جزوه, مخملی, سوروس, کودتا مخملی, انصار الله, جنبش انصار, مدرسین, عبدالملک الحوثی, الحوثی, عبدالملک</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Title  \\\n",
       "0    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم   \n",
       "1    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم   \n",
       "2    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم   \n",
       "3                              آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را جمع آوری کرد   \n",
       "4                              آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را جمع آوری کرد   \n",
       "..                                                                                     ...   \n",
       "108                      گروگانگیری در شعبه بانک انصار شوش با دستگیری گروگانگیر پایان یافت   \n",
       "109                          هشدار رئیس‌جمهور چین نسبت به تهدید‌های فزاینده علیه امنیت ملی   \n",
       "110                                                 آمریکا برای حمله به سوریه آماده می‌شود   \n",
       "111                        پنتاگون در آستانه تعطیل دولت، ۵ میلیارد دلار تجهیزات نظامی خرید   \n",
       "112  مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می‌کنیم/صنعا: از حمایت‌های آمریکا متشکریم   \n",
       "\n",
       "                                   Class  \\\n",
       "0                                اجتماعی   \n",
       "1                               اطلاعاتی   \n",
       "2                 جو فرهنگی حاکم بر کشور   \n",
       "3                          فرهنگی امنیتی   \n",
       "4               تهدیدات اجتماعی و فرهنگی   \n",
       "..                                   ...   \n",
       "108                 مبارزه با گروگانگیری   \n",
       "109  مسولان عالی تصمیم گیر کشوری و لشگری   \n",
       "110                         پدافند هوایی   \n",
       "111     عملیات روانی و تهاجم فرهنگی دشمن   \n",
       "112               فعالیت احزاب و گروه ها   \n",
       "\n",
       "                                                                                                                           Key words  \n",
       "0                                       آسیب اجتماع, بچه پولدار, سبک زندگی, مخملی, جنگ نرم, اعتیاد, سوروس, پولدار, بافق, کودتا مخملی  \n",
       "1                                           یوتا, موادمخدر, جزوه, بلک واتر, ابوطالب, بچه پولدار, برنامه اقدام, سالیوان, آستارا, واتر  \n",
       "2                              بچه پولدار, پولدار, مشایی, انصار الله, عبدالملک الحوثی, جنبش انصار, حجاب, الحوثی, جمهور یمن, شهر صنعا  \n",
       "3                                       آسیب اجتماع, بچه پولدار, مخملی, سبک زندگی, جنگ نرم, سوروس, اعتیاد, پولدار, کودتا مخملی, بافق  \n",
       "4                             آسیب اجتماع, بچه پولدار, اعتیاد, پولدار, تهاجم فرهنگ, موادمخدر, التحریر, نرخ بیکاری, ازدواج, سبک زندگی  \n",
       "..                                                                                                                               ...  \n",
       "108                        آزاد مرزبان, گروه جیش, جیش العدل, جمشید دانا, دانا فر, العدل, ذوالفقار, مرزبان ربوده, جمشید, مرزبان ایران  \n",
       "109              فاینانس, انصار الله, عبدالملک الحوثی, آمرلی, جنبش انصار, دریابانی, مرزبان ناجا, قرارگاه پدافند, جمهور یمن, پوروشنکو  \n",
       "110  قرارگاه پدافند, موشک اروپا, هوا خاتم, فرمانده قرارگاه, دادستان آلمان, پیامبر اعظم, ایران, خاتم الانبیاء, الانبیاء, سامانه رادار  \n",
       "111                                        جزوه, مدرسین, تهاجم فرهنگ, مشایی, بختیار, مصباح یزد, ایران, التحریر, حزب مشارکت, علمیه قم  \n",
       "112                               جزوه, مخملی, سوروس, کودتا مخملی, انصار الله, جنبش انصار, مدرسین, عبدالملک الحوثی, الحوثی, عبدالملک  \n",
       "\n",
       "[113 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust display settings\n",
    "pd.set_option('display.max_colwidth', None)  # For pandas versions >= 1.0\n",
    "# For older versions of pandas, use 'display.max_colwidth', -1\n",
    "\n",
    "data_samples = []\n",
    "for index, category in enumerate(categories):\n",
    "    value = df_tfidf.iloc[index, :]\n",
    "    contents = content_by_category[category]\n",
    "    top_scores = value.sort_values(ascending=False).head(10)\n",
    "    top_words = top_scores.index\n",
    "    \n",
    "    # Convert the index object 'top_words' to a string separated by commas\n",
    "    top_words_str = ', '.join(top_words)\n",
    "    \n",
    "    title = contents['titles'][0]\n",
    "    \n",
    "    # Append the 'top_words_str' instead of 'top_words' index object\n",
    "    data_samples.append({\"Title\": title, \"Class\": category, \"Key words\": top_words_str})\n",
    "\n",
    "df_samples = pd.DataFrame(data_samples)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>آزاد</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آغاز</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آماده</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آمد</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آمریکا</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>بیانیه</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>بین</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>تاریخ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>تامین</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>تاکید</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  IDF Score\n",
       "0     آزاد        1.0\n",
       "1     آغاز        1.0\n",
       "2    آماده        1.0\n",
       "3      آمد        1.0\n",
       "4   آمریکا        1.0\n",
       "..     ...        ...\n",
       "95  بیانیه        1.0\n",
       "96     بین        1.0\n",
       "97   تاریخ        1.0\n",
       "98   تامین        1.0\n",
       "99   تاکید        1.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'tfidf_vectorizer' is your TfidfVectorizer instance and it's already been fit to your dataset\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_scores = tfidf_vectorizer.idf_\n",
    "\n",
    "# Pairing feature names with their IDF scores and sorting by IDF score in ascending order\n",
    "sorted_features_by_idf = sorted(zip(feature_names, idf_scores), key=lambda x: x[1])\n",
    "\n",
    "# Selecting the top N words with the lowest IDF scores\n",
    "top_n = 100\n",
    "lowest_idf_words = sorted_features_by_idf[:top_n]\n",
    "\n",
    "# Creating a DataFrame from the top N lowest IDF scores\n",
    "df_lowest_idf = pd.DataFrame(lowest_idf_words, columns=['Word', 'IDF Score'])\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_lowest_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = categories\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 10000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb_model = nb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
