{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_directory = \"./news_dataset\"\n",
    "\n",
    "unique_docs_per_category = {}\n",
    "global_seen_docs = {}\n",
    "untitled_folder_count = 0\n",
    "unique_files = set()\n",
    "all_files = []\n",
    "content_by_category = {}\n",
    "for root, dirs, files in os.walk(root_directory):\n",
    "    if os.path.basename(root) == \"Untitled Folder\":\n",
    "        untitled_folder_count += 1\n",
    "        continue\n",
    "    relative_path = os.path.relpath(root, root_directory)\n",
    "    path_components = [\"Root\"] if relative_path == \".\" else relative_path.split(os.sep)\n",
    "    for component in path_components:\n",
    "        if component == \"Untitled Folder\" or component == \"Root\":\n",
    "            continue\n",
    "        if component not in unique_docs_per_category:\n",
    "            unique_docs_per_category[component] = 0\n",
    "            global_seen_docs[component] = set()\n",
    "            content_by_category[component] = {'titles': [], 'bodies': []}\n",
    "    files.sort(key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    for file in files:\n",
    "        unique_files.add(file)\n",
    "        all_files.append(file)\n",
    "        file_path = os.path.join(root, file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            title = lines[0].strip()\n",
    "            body = ''.join(lines[1:]).strip()\n",
    "        for category in path_components:\n",
    "            if category != \"Untitled Folder\" and file not in global_seen_docs[category]:\n",
    "                unique_docs_per_category[category] += 1\n",
    "                global_seen_docs[category].add(file)\n",
    "                content_by_category[category]['titles'].append(title)\n",
    "                content_by_category[category]['bodies'].append(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 113\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of categories: \" + str(len(unique_docs_per_category.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 17599\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents: \" + str(len(all_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique documents: 6539\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique documents: \" + str(len(unique_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Class</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>اجتماعی</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>اطلاعاتی</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>جو فرهنگی حاکم بر کشور</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...</td>\n",
       "      <td>فرهنگی امنیتی</td>\n",
       "      <td>تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...</td>\n",
       "      <td>تهدیدات اجتماعی و فرهنگی</td>\n",
       "      <td>تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>گروگانگیری در شعبه بانک انصار شوش با دستگیری گ...</td>\n",
       "      <td>مبارزه با گروگانگیری</td>\n",
       "      <td>دزفول- ایرنا- فرد مسلحی که صبح پنجشنبه با ورود...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>هشدار رئیس‌جمهور چین نسبت به تهدید‌های فزاینده...</td>\n",
       "      <td>مسولان عالی تصمیم گیر کشوری و لشگری</td>\n",
       "      <td>رییس‌جمهوری چین با تاکید بر اینکه چین با تهدید...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>آمریکا برای حمله به سوریه آماده می‌شود</td>\n",
       "      <td>پدافند هوایی</td>\n",
       "      <td>وزیر دفاع آمریکا اعلام کرد، پنتاگون در حال است...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>پنتاگون در آستانه تعطیل دولت، ۵ میلیارد دلار ت...</td>\n",
       "      <td>عملیات روانی و تهاجم فرهنگی دشمن</td>\n",
       "      <td>ب پیش از فرستاده شدن ۸۰۰ هزار کارمند دولت آمری...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...</td>\n",
       "      <td>فعالیت احزاب و گروه ها</td>\n",
       "      <td>خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "1    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "2    مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "3    آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...   \n",
       "4    آمریکا پنج میلیارد رکورد اطلاعاتی ایرانی ها را...   \n",
       "..                                                 ...   \n",
       "108  گروگانگیری در شعبه بانک انصار شوش با دستگیری گ...   \n",
       "109  هشدار رئیس‌جمهور چین نسبت به تهدید‌های فزاینده...   \n",
       "110             آمریکا برای حمله به سوریه آماده می‌شود   \n",
       "111  پنتاگون در آستانه تعطیل دولت، ۵ میلیارد دلار ت...   \n",
       "112  مشاور اوباما: تجمع حوثی‌ها در صنعا را محکوم می...   \n",
       "\n",
       "                                   Class  \\\n",
       "0                                اجتماعی   \n",
       "1                               اطلاعاتی   \n",
       "2                 جو فرهنگی حاکم بر کشور   \n",
       "3                          فرهنگی امنیتی   \n",
       "4               تهدیدات اجتماعی و فرهنگی   \n",
       "..                                   ...   \n",
       "108                 مبارزه با گروگانگیری   \n",
       "109  مسولان عالی تصمیم گیر کشوری و لشگری   \n",
       "110                         پدافند هوایی   \n",
       "111     عملیات روانی و تهاجم فرهنگی دشمن   \n",
       "112               فعالیت احزاب و گروه ها   \n",
       "\n",
       "                                                  Body  \n",
       "0    خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "1    خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "2    خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "3    تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...  \n",
       "4    تهران- ایرنا- اطلاعات حاکی از آن است که آمریکا...  \n",
       "..                                                 ...  \n",
       "108  دزفول- ایرنا- فرد مسلحی که صبح پنجشنبه با ورود...  \n",
       "109  رییس‌جمهوری چین با تاکید بر اینکه چین با تهدید...  \n",
       "110  وزیر دفاع آمریکا اعلام کرد، پنتاگون در حال است...  \n",
       "111  ب پیش از فرستاده شدن ۸۰۰ هزار کارمند دولت آمری...  \n",
       "112  خبرگزاری تسنیم: مشاور ارشد رئیس جمهور آمریکا د...  \n",
       "\n",
       "[113 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `content_by_category` is the dictionary you're working with\n",
    "data_samples = []\n",
    "for category in content_by_category.keys():\n",
    "    contents = content_by_category[category]\n",
    "    title, body = contents['titles'][0],contents['bodies'][0]\n",
    "    displayed_body = (body[:47] + '...') if len(body) > 50 else body\n",
    "    data_samples.append({\"Title\": title, \"Class\": category, \"Body\": displayed_body})\n",
    "\n",
    "df_samples = pd.DataFrame(data_samples)\n",
    "\n",
    "display(df_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import hazm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "normalizer = hazm.Normalizer()\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "punctuations = [')', '(', '>', '<', \"؛\", \"،\", '{', '}', \"؟\", ':', \"–\", '»', '\"', '«', '[', ']', '\"', '+', '=', '?', '/',\n",
    "                '//', '\\\\', '|', '!', '%', '&', '*', '$', '#', '؟', '*', '.', '_', '']\n",
    "preprocessed_docs = []\n",
    "categories = []\n",
    "stopwords_list = hazm.stopwords_list()\n",
    "for category in content_by_category.keys():\n",
    "    category_text = \"\"\n",
    "    for doc in content_by_category[category]['bodies']:\n",
    "        normalized_text = normalizer.normalize(doc)\n",
    "        tokens = hazm.word_tokenize(normalized_text)\n",
    "        lemmatized_text = \"\"\n",
    "        for token in tokens:\n",
    "            if token in stopwords_list or token in punctuations:\n",
    "                continue\n",
    "            lemmatized_token = lemmatizer.lemmatize(token)\n",
    "            lemma = lemmatized_token.split('#')[1] if '#' in lemmatized_token else lemmatized_token\n",
    "            lemmatized_text += lemmatized_token + \" \"\n",
    "        category_text += lemmatized_text + \" \"\n",
    "\n",
    "    preprocessed_docs.append(category_text.strip())\n",
    "    categories.append(category)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,  ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m),sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mpreprocessed_docs\u001b[49m)\n\u001b[0;32m     10\u001b[0m df_tfidf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_tfidf)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import hazm\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000,  ngram_range=(1,2),sublinear_tf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "آباد\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust display settings\n",
    "pd.set_option('display.max_colwidth', None)  # For pandas versions >= 1.0\n",
    "# For older versions of pandas, use 'display.max_colwidth', -1\n",
    "\n",
    "data_samples = []\n",
    "for index, category in enumerate(categories):\n",
    "    value = df_tfidf.iloc[index, :]\n",
    "    contents = content_by_category[category]\n",
    "    top_scores = value.sort_values(ascending=False).head(10)\n",
    "    top_words = top_scores.index\n",
    "    \n",
    "    # Convert the index object 'top_words' to a string separated by commas\n",
    "    top_words_str = ', '.join(top_words)\n",
    "    \n",
    "    title = contents['titles'][0]\n",
    "    \n",
    "    # Append the 'top_words_str' instead of 'top_words' index object\n",
    "    data_samples.append({\"Title\": title, \"Class\": category, \"Key words\": top_words_str})\n",
    "\n",
    "df_samples = pd.DataFrame(data_samples)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 TF-IDF Scores for the First Document:\n",
      "ایران: 0.47314137847428506\n",
      "کن: 0.3782210402001229\n",
      "شو: 0.303014926029056\n",
      "کشور: 0.24387225371977037\n",
      "گو: 0.1467614461008199\n",
      "گزارش: 0.1423805074112432\n",
      "ده: 0.11828534461857126\n",
      "باش: 0.11755518817030848\n",
      "مخدر: 0.11253964667190496\n",
      "مواد: 0.10626042693172154\n",
      "سال: 0.09857112051547605\n",
      "قاچاق: 0.09330738473932354\n",
      "موادمخدر: 0.09270612990895831\n",
      "مرز: 0.08542830444674591\n",
      "آمریکا: 0.08396799155022033\n",
      "نیرو: 0.08031720930890641\n",
      "نظام: 0.07666642706759248\n",
      "ترکیه: 0.07374580127454135\n",
      "پاکستان: 0.0709501071871688\n",
      "سوریه: 0.07009501903322742\n",
      "امنیت: 0.07009501903322742\n",
      "منطقه: 0.06790454968843905\n",
      "گیر: 0.06425376744712513\n",
      "دولت: 0.06425376744712513\n",
      "توان: 0.06425376744712513\n",
      "همکار: 0.06425376744712513\n",
      "سپاه: 0.06279345455059956\n",
      "سلاح: 0.06133314165407399\n",
      "اسلام: 0.0606029852058112\n",
      "افغانستان: 0.059872828757548416\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'tfidf_vectorizer' is your TfidfVectorizer instance and it's already been fit to your dataset\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_scores = tfidf_vectorizer.idf_\n",
    "\n",
    "# Pairing feature names with their IDF scores and sorting by IDF score in ascending order\n",
    "sorted_features_by_idf = sorted(zip(feature_names, idf_scores), key=lambda x: x[1])\n",
    "\n",
    "# Selecting the top N words with the lowest IDF scores\n",
    "top_n = 100\n",
    "lowest_idf_words = sorted_features_by_idf[:top_n]\n",
    "\n",
    "# Creating a DataFrame from the top N lowest IDF scores\n",
    "df_lowest_idf = pd.DataFrame(lowest_idf_words, columns=['Word', 'IDF Score'])\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_lowest_idf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
